{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMoUl7ACaZNfVavX66N4rb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoungJae20213137/24_Lab1_1/blob/main/MNIST2_Data_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "id": "TGzo6YzLOORO",
        "outputId": "507d0d01-8df8-4b8b-e461-fb300eb69349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/train-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 489kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/train-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.50MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.75MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
            "\n",
            "torch.Size([60000, 28, 28])\n",
            "torch.Size([10000, 28, 28])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'_SingleProcessDataLoaderIter' object has no attribute 'next'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-aab12ae1d5b1>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mdata_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_SingleProcessDataLoaderIter' object has no attribute 'next'"
          ]
        }
      ],
      "source": [
        "# https://towardsdatascience.com/start-your-cnn-journey-with-pytorch-in-python-6f8364a79801\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# data preparation\n",
        "# Transform the data to torch tensors and normalize it\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(\n",
        "                                    (0.5, ), (0.5, ))])\n",
        "\n",
        "# Prepare training set and testing set\n",
        "trainset = torchvision.datasets.MNIST('mnist', train=True,\n",
        "                                      download=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST('mnist', train=False,\n",
        "                                     download=True, transform=transform)\n",
        "\n",
        "# Prepare training loader and testing loader\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=0)\n",
        "\n",
        "print(trainset.data.shape)\n",
        "print(testset.data.shape)\n",
        "\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = data_iter.next()\n",
        "\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "figure = plt.figure()\n",
        "num_of_images = 40\n",
        "for index in range(1, num_of_images + 1):\n",
        "    plt.subplot(4, 10, index)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(trainset.data[index], cmap='gray_r')\n",
        "\n",
        "# preparing the model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Declare all the layers for feature extraction\n",
        "        self.features = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, stride=1, padding=1),\n",
        "                                      nn.ReLU(inplace=True),\n",
        "                                      nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
        "                                      nn.MaxPool2d(2, 2),\n",
        "                                      nn.ReLU(inplace=True),\n",
        "                                      nn.BatchNorm2d(10),\n",
        "                                      nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, stride=1, padding=1),\n",
        "                                      nn.ReLU(inplace=True),\n",
        "                                      nn.BatchNorm2d(20),\n",
        "                                      nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, stride=1, padding=1),\n",
        "                                      nn.MaxPool2d(2, 2),\n",
        "                                      nn.ReLU(inplace=True),\n",
        "                                      nn.BatchNorm2d(40))\n",
        "\n",
        "        # Declare all the layers for classification\n",
        "        self.classifier = nn.Sequential(nn.Linear(7 * 7 * 40, 200),\n",
        "                                        nn.ReLU(inplace=True),\n",
        "                                        nn.Dropout(p=0.5),\n",
        "                                        nn.Linear(200, 500),\n",
        "                                        nn.ReLU(inplace=True),\n",
        "                                        nn.Linear(500, 10))\n",
        "\n",
        "        def forward(self, x):\n",
        "\n",
        "            # Apply the feature extractor in the input\n",
        "            x = self.features(x)\n",
        "\n",
        "            # Squeeze the three spatial dimensions in one\n",
        "            x = x.view(-1, 7 * 7 * 40)\n",
        "\n",
        "            # Classify the images\n",
        "            x = self.classifier(x)\n",
        "            return x\n",
        "\n",
        "# print model architecture\n",
        "model = Net()\n",
        "print(model)\n",
        "\n",
        "# Loss criteria and optimizer\n",
        "# Instantiate the network\n",
        "model = Net()\n",
        "\n",
        "# Instantiate the cross-entropy loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Instantiate the Adam optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.001)\n",
        "\n",
        "# Model training\n",
        "# batch_size, epoch and iteration\n",
        "batch_size = 100\n",
        "features_train = trainset.data.shape[0]\n",
        "n_iters = 6000\n",
        "\n",
        "num_epochs = n_iters/(features_train/batch_size)\n",
        "\n",
        "num_epochs = int(num_epochs)\n",
        "num_epochs\n",
        "\n",
        "# CNN model training\n",
        "count = 0\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        train, labels = data\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward propagation\n",
        "        outputs = model(train)\n",
        "\n",
        "        # Calculate relu and cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Calculating gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        if count % 50 == 0:\n",
        "            # Calculate Accuracy\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for i, data in enumerate(test_loader, 0):\n",
        "                test, labels = data\n",
        "\n",
        "                # Forward propagation\n",
        "                outputs = model(test)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                predicted = torch.max(outputs.data, 1)[1]\n",
        "\n",
        "                # Total number of labels\n",
        "                total += len(labels)\n",
        "\n",
        "                correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct / float(total)\n",
        "\n",
        "            # store loss value and iteration\n",
        "            loss_list.append(loss.data)\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "        if count % 600 == 0:\n",
        "            # Print Loss\n",
        "            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))\n",
        "\n",
        "# Visualize Loss and Accuracy\n",
        "plt.plot(iteration_list, loss_list)\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"CNN: Loss vs Number of iteration\")\n",
        "plt.show()\n",
        "\n",
        "# Visualization accuracy\n",
        "plt.plot(iteration_list, accuracy_list, color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"CNN: Accuracy vs Number of Iteration\")\n",
        "plt.show"
      ]
    }
  ]
}