{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN3HX+5RRUI/8U49NANsFYA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoungJae20213137/24_Lab1_1/blob/main/MNIST2_Data_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGzo6YzLOORO",
        "outputId": "c59fc712-4929-4b37-de43-cf4e792fa967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "torch.Size([10000, 28, 28])\n",
            "torch.Size([100, 1, 28, 28])\n",
            "torch.Size([100])\n",
            "Net(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(5, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): Conv2d(20, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=1960, out_features=200, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=200, out_features=500, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Linear(in_features=500, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Iteration: 600  Loss: 0.037452176213264465  Accuracy: 97.8499984741211 %\n",
            "Iteration: 1200  Loss: 0.08369357883930206  Accuracy: 98.23999786376953 %\n",
            "Iteration: 1800  Loss: 0.039793480187654495  Accuracy: 98.58000183105469 %\n",
            "Iteration: 2400  Loss: 0.0033241368364542723  Accuracy: 98.5999984741211 %\n"
          ]
        }
      ],
      "source": [
        "# https://towardsdatascience.com/start-your-cnn-journey-with-pytorch-in-python-6f8364a79801\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# data preparation\n",
        "# Transform the data to torch tensors and normalize it\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(\n",
        "                                    (0.5, ), (0.5, ))])\n",
        "\n",
        "# Prepare training set and testing set\n",
        "trainset = torchvision.datasets.MNIST('mnist', train=True,\n",
        "                                      download=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST('mnist', train=False,\n",
        "                                     download=True, transform=transform)\n",
        "\n",
        "# Prepare training loader and testing loader\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=0)\n",
        "\n",
        "print(trainset.data.shape)\n",
        "print(testset.data.shape)\n",
        "\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "figure = plt.figure()\n",
        "num_of_images = 40\n",
        "for index in range(1, num_of_images + 1):\n",
        "    plt.subplot(4, 10, index)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(trainset.data[index], cmap='gray_r')\n",
        "\n",
        "# preparing the model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Declare all the layers for feature extraction\n",
        "        self.features = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, stride=1, padding=1),\n",
        "                                      nn.ReLU(inplace=True),\n",
        "                                      nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
        "                                      nn.MaxPool2d(2, 2),\n",
        "                                      nn.ReLU(inplace=True),\n",
        "                                      nn.BatchNorm2d(10),\n",
        "                                      nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, stride=1, padding=1),\n",
        "                                      nn.ReLU(inplace=True),\n",
        "                                      nn.BatchNorm2d(20),\n",
        "                                      nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, stride=1, padding=1),\n",
        "                                      nn.MaxPool2d(2, 2),\n",
        "                                      nn.ReLU(inplace=True),\n",
        "                                      nn.BatchNorm2d(40))\n",
        "\n",
        "        # Declare all the layers for classification\n",
        "        self.classifier = nn.Sequential(nn.Linear(7 * 7 * 40, 200),\n",
        "                                        nn.ReLU(inplace=True),\n",
        "                                        nn.Dropout(p=0.5),\n",
        "                                        nn.Linear(200, 500),\n",
        "                                        nn.ReLU(inplace=True),\n",
        "                                        nn.Linear(500, 10))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Apply the feature extractor in the input\n",
        "        x = self.features(x)\n",
        "\n",
        "        # Squeeze the three spatial dimensions in one\n",
        "        x = x.view(-1, 7 * 7 * 40)\n",
        "\n",
        "        # Classify the images\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# print model architecture\n",
        "model = Net()\n",
        "print(model)\n",
        "\n",
        "# Loss criteria and optimizer\n",
        "# Instantiate the network\n",
        "model = Net()\n",
        "\n",
        "# Instantiate the cross-entropy loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Instantiate the Adam optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.001)\n",
        "\n",
        "# Model training\n",
        "# batch_size, epoch and iteration\n",
        "batch_size = 100\n",
        "features_train = trainset.data.shape[0]\n",
        "n_iters = 6000\n",
        "\n",
        "num_epochs = n_iters/(features_train/batch_size)\n",
        "\n",
        "num_epochs = int(num_epochs)\n",
        "num_epochs\n",
        "\n",
        "# CNN model training\n",
        "count = 0\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        train, labels = data\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward propagation\n",
        "        outputs = model(train)\n",
        "\n",
        "        # Calculate relu and cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Calculating gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        if count % 50 == 0:\n",
        "            # Calculate Accuracy\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for i, data in enumerate(test_loader, 0):\n",
        "                test, labels = data\n",
        "\n",
        "                # Forward propagation\n",
        "                outputs = model(test)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                predicted = torch.max(outputs.data, 1)[1]\n",
        "\n",
        "                # Total number of labels\n",
        "                total += len(labels)\n",
        "\n",
        "                correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct / float(total)\n",
        "\n",
        "            # store loss value and iteration\n",
        "            loss_list.append(loss.data)\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "        if count % 600 == 0:\n",
        "            # Print Loss\n",
        "            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))\n",
        "\n",
        "# Visualize Loss and Accuracy\n",
        "plt.plot(iteration_list, loss_list)\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"CNN: Loss vs Number of iteration\")\n",
        "plt.show()\n",
        "\n",
        "# Visualization accuracy\n",
        "plt.plot(iteration_list, accuracy_list, color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"CNN: Accuracy vs Number of Iteration\")\n",
        "plt.show"
      ]
    }
  ]
}